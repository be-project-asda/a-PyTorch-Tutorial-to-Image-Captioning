{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BDOgpjzg0vDb"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cBbM__xpqrQJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import skimage.transform\n",
    "import argparse\n",
    "from scipy.misc import imread, imresize\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GK2dad12qzDT"
   },
   "outputs": [],
   "source": [
    "def __add_diversity(\n",
    "        beam_seqs,\n",
    "        logprobs,\n",
    "        time,\n",
    "        group_number,\n",
    "        penalty_lambda,\n",
    "        group_size):\n",
    "    \"\"\" Adds a diversity penalty to a group of beams\n",
    "    beam_seqs       : array containing beam sequences staggered across time\n",
    "    logprobs        : log probabilities for the beam sequences\n",
    "    time            : Current time unit (not adjusted for the current group\n",
    "    group_number    : the current group number\n",
    "    penalty_lambda  : diversity penalty\n",
    "    group_size      : num_beams/num_groups\n",
    "    \"\"\"\n",
    "    local_time = time - group_number # current time for the group\n",
    "    aug_logprobs = logprobs.clone().to(device)\n",
    "    for previous_choice in range(group_number):\n",
    "        previous_decisions = beam_seqs[previous_choice][local_time]\n",
    "        for beam in range(group_size):\n",
    "            for previous_labels in range(group_size):\n",
    "                aug_logprobs[beam][previous_decisions[previous_labels]] -= (\n",
    "                    penalty_lambda\n",
    "                ) # penalize previously chosen words\n",
    "    return aug_logprobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uJ506tf2rSDg"
   },
   "outputs": [],
   "source": [
    "def __beam_step(\n",
    "        logprobs,\n",
    "        aug_logprobs,\n",
    "        beam_size,\n",
    "        beam_seq,\n",
    "        beam_seq_logprobs,\n",
    "        beam_logprobs_sum,\n",
    "        rnn_state,\n",
    "        time):\n",
    "    \"\"\" Runs one step of beam search\n",
    "    logprobs         : log probabilities for beam_seqs\n",
    "    aug_logprobs     : log probabilities after penalty\n",
    "    beam_size        : Beam Size\n",
    "    beam_seq         : Tensor containing the beams\n",
    "    beam_seq_logprobs: log-probabilities of each beam sequence\n",
    "    beam_logprobs_sum: joint log probability of each beam\n",
    "    time             : time step\n",
    "    \"\"\"\n",
    "#     print(\"aug log probs size: \", aug_logprobs.shape)\n",
    "    ys, indices = torch.sort(aug_logprobs, 1, True)\n",
    "    candidates = []\n",
    "    columns = min(beam_size, len(ys[0]))\n",
    "    rows = beam_size\n",
    "#     print(beam_size)\n",
    "    if time == 1:\n",
    "        rows = 1\n",
    "\n",
    "    for column in range(columns):\n",
    "        for row in range(rows):\n",
    "            local_logprob = ys[row,column]\n",
    "            candidate_logprob = beam_logprobs_sum[row] + local_logprob\n",
    "            local_unaugmented_logprob = logprobs[row,indices[row][column]]\n",
    "            candidates.append(\n",
    "                {\n",
    "                    \"column\":indices[row][column],\n",
    "                    \"row\":row,\n",
    "                    \"prob\":candidate_logprob,\n",
    "                    \"local_logprob\":local_unaugmented_logprob\n",
    "                }\n",
    "            )\n",
    "    candidates.sort(key=lambda x: x['prob'])\n",
    "#     print(rnn_state)\n",
    "    new_state = [x.clone().to(device) for x in rnn_state]\n",
    "#     print(\"new state: \", len(new_state))\n",
    "#     print(\"rnn_state: \", len(rnn_state), type(rnn_state[0]), rnn_state[0].shape)\n",
    "#     print(\"candidates: \", len(candidates))\n",
    "\n",
    "    if time > 1:\n",
    "        beam_seq_prev = beam_seq[0:time,:].clone()\n",
    "        beam_seq_logprobs_prev = beam_seq_logprobs[0:time, :].clone()\n",
    "\n",
    "    for v_index in range(beam_size):\n",
    "        candidate = candidates[v_index]\n",
    "        candidate['kept'] = True\n",
    "        if time > 1:\n",
    "            beam_seq[0:time, v_index] = (\n",
    "                beam_seq_prev[:,candidate['row']]\n",
    "            )\n",
    "            beam_seq_logprobs[0:time, v_index] = (\n",
    "                beam_seq_logprobs_prev[:,candidate['row']]\n",
    "            )\n",
    "        for state_index in range(len(new_state)):\n",
    "#             print(\"\\n\\n\\n\",v_index)\n",
    "            new_state[state_index][v_index] = (\n",
    "                rnn_state[state_index][candidate['row']]\n",
    "            )\n",
    "\n",
    "        beam_seq[time,v_index] = candidate['column']\n",
    "        beam_seq_logprobs[time, v_index] = candidate['local_logprob']\n",
    "        beam_logprobs_sum[v_index] = candidate['prob']\n",
    "    state = new_state\n",
    "    return (\n",
    "        beam_seq,\n",
    "        beam_seq_logprobs,\n",
    "        beam_logprobs_sum,\n",
    "        state,\n",
    "        candidates\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9cRig8zXrUGk"
   },
   "outputs": [],
   "source": [
    "def diverse_beam_search(\n",
    "        rnn_state,\n",
    "        init_logprobs,\n",
    "        num_beams,\n",
    "        num_groups,\n",
    "        penalty_lambda,\n",
    "        seq_length,\n",
    "        end_token,\n",
    "        gen_logprobs,\n",
    "        init_token):\n",
    "    \"\"\" Performs Diverse Beam Search\n",
    "    seq_length: maximum length of sequence\n",
    "    rnn_state: states of the RNNs\n",
    "    logprobs: log-probabilities of the beams\n",
    "    num_beams: number of beams\n",
    "    num_groups: number of groups\n",
    "    penalty_lambda: value of diversity penalty\n",
    "    end_token: end-token of the vocabulary\n",
    "    gen_logprobs: function that returns\n",
    "    the states and logprobs from the RNN output\n",
    "    \"\"\"\n",
    "    beam_ratio = int(num_beams / num_groups)\n",
    "    states = []\n",
    "    beam_seqs = []\n",
    "    beam_seq_logprobs = []\n",
    "    beam_logprobs_sums = []\n",
    "    done_beams = []\n",
    "    logprobs_list=[]\n",
    "    state=[]\n",
    "    # Initialization\n",
    "#     print(\"BEAM RATIO \", beam_ratio, \"\\n\", type(beam_ratio))\n",
    "\n",
    "    state = [None]*beam_ratio\n",
    "#     print(state)\n",
    "\n",
    "    for group_num in range(num_groups):\n",
    "        beam_seqs.append(torch.zeros(\n",
    "            seq_length, beam_ratio, dtype=torch.long\n",
    "        ).to(device))\n",
    "\n",
    "        beam_seq_logprobs.append(torch.zeros(\n",
    "            seq_length, beam_ratio\n",
    "        ).to(device))\n",
    "\n",
    "        beam_logprobs_sums.append(torch.zeros(\n",
    "            beam_ratio\n",
    "        ).to(device))\n",
    "\n",
    "        for k in range(beam_ratio):\n",
    "            done_beams.append(None)\n",
    "        logprobs_list.append(torch.zeros(\n",
    "            beam_ratio, init_logprobs.size()[1]\n",
    "        ).to(device))\n",
    "\n",
    "        logprobs_list[group_num] = (\n",
    "            init_logprobs.clone().to(device)\n",
    "        )\n",
    "\n",
    "        for sub_beam_ix in range(beam_ratio):\n",
    "\n",
    "            beam_seqs[group_num][sub_beam_ix][0] = init_token\n",
    "            state[sub_beam_ix] = [st.clone().to(device) for st in rnn_state]\n",
    "\n",
    "#             print(state)\n",
    "\n",
    "        states.append(\n",
    "            [item.clone().to(device) for item in rnn_state]\n",
    "        )\n",
    "    # End initialization\n",
    "\n",
    "    for time in range(seq_length + num_groups):\n",
    "        for group_ix in range(num_groups):\n",
    "            if time >= group_ix and time < seq_length + group_ix:\n",
    "                logprobs = logprobs_list[group_ix]\n",
    "\n",
    "                # Suppress <UNK> tokens in the decoding\n",
    "                logprobs[:,-2] -= 1000\n",
    "#                 print(\"logprobs shape: \", logprobs.shape)\n",
    "                aug_logprobs = __add_diversity(\n",
    "                    beam_seqs,\n",
    "                    logprobs,\n",
    "                    time,\n",
    "                    group_ix,\n",
    "                    penalty_lambda,\n",
    "                    beam_ratio\n",
    "                )\n",
    "\n",
    "                # Runs one step of beam_search for the current group\n",
    "                (\n",
    "                    beam_seqs[group_ix],\n",
    "                    beam_seq_logprobs[group_ix],\n",
    "                    beam_logprobs_sums[group_ix],\n",
    "                    states[group_ix],\n",
    "                    candidates_group\n",
    "                ) = __beam_step(\n",
    "                        logprobs,\n",
    "                        aug_logprobs,\n",
    "                        beam_ratio,\n",
    "                        beam_seqs[group_ix],\n",
    "                        beam_seq_logprobs[group_ix],\n",
    "                        beam_logprobs_sums[group_ix],\n",
    "                        states[group_ix],\n",
    "                        time - group_ix\n",
    "                )\n",
    "\n",
    "                for beam_ix in range(beam_ratio):\n",
    "                    is_first_end_token = (\n",
    "                        (\n",
    "                            beam_seqs[group_ix][:,beam_ix][time-group_ix] == \\\n",
    "                         end_token\n",
    "                        ) and (\n",
    "                            torch.eq(\n",
    "                                beam_seqs[group_ix][:,beam_ix],\n",
    "                                end_token\n",
    "                            ).sum() == 1\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    final_time_wo_end = (\n",
    "                        (\n",
    "                            time == seq_length + group_ix\n",
    "                        ) and (\n",
    "                            torch.eq(\n",
    "                                beam_seqs[group_ix][:,beam_ix],\n",
    "                                end_token\n",
    "                            ).sum() == 0\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    if is_first_end_token or final_time_wo_end:\n",
    "#                         print(\"End token reached!\")\n",
    "#                         print(\"time: {}, group_ix: {}, beam_index: {}\".format(\n",
    "#                             time, group_ix, beam_ix\n",
    "#                         ))\n",
    "                        final_beam = {\n",
    "                            \"seq\": beam_seqs[group_ix][:,beam_ix].clone(),\n",
    "                            \"logps\": beam_seq_logprobs[group_ix]\\\n",
    "                                [:,beam_ix].clone(),\n",
    "                            \"logp\": beam_seq_logprobs[group_ix][:,beam_ix]\\\n",
    "                                .sum(),\n",
    "                            \"aug_logp\": beam_logprobs_sums[group_ix][beam_ix]\n",
    "                        }\n",
    "                        final_beam[\"candidate\"] = candidates_group[beam_ix]\n",
    "#                         print([rev_word_map[int(word)] for word in final_beam['seq']])\n",
    "                        done_beams[group_ix*beam_ratio+beam_ix] = final_beam\n",
    "\n",
    "                    if is_first_end_token:\n",
    "                        beam_logprobs_sums[group_ix][beam_ix] = -1000000\n",
    "\n",
    "                inpt = beam_seqs[group_ix][time - group_ix]\n",
    "                output = gen_logprobs(inpt, states[group_ix])\n",
    "#                 print(\"Type of output: {}\".format(type(output)))\n",
    "#                 for indexofiteminoutput, iteminoutput in enumerate(output):\n",
    "#                     print(\"type of output[{}]: {}\".format(\n",
    "#                         indexofiteminoutput, type(iteminoutput)\n",
    "#                     ))\n",
    "#                     print(\"Shape of output[{}]: {}\".format(\n",
    "#                         indexofiteminoutput, iteminoutput.size()\n",
    "#                     ))\n",
    "#                 print(\"Shape of Last element in output: {}\".format(output[-1].size()))\n",
    "                logprobs_list[group_ix] = output[-1].clone()\n",
    "                temp_state = [\n",
    "                    output[i].clone().to(device)\n",
    "                    for i\n",
    "                    in range(len(output)-1)\n",
    "                ]\n",
    "                states[group_ix] = [st.clone().to(device) for st in temp_state]\n",
    "\n",
    "#     outputs = []\n",
    "#     for i in range(num_groups):\n",
    "#         curr_outs = [[] for s in range(beam_ratio)]\n",
    "#         group = beam_seqs[i]\n",
    "#         print(group.size())\n",
    "#         for j in range(group.size()[1]):\n",
    "#             for k in range(group.size()[0]):\n",
    "#                 curr_outs[j].append(rev_word_map[int(group[k,j])])\n",
    "#         outputs.extend(curr_outs)\n",
    "#     input()\n",
    "#     for op in outputs:\n",
    "#         print(op)\n",
    "#     input()\n",
    "#     for i in range(num_groups):\n",
    "#         done_beams[i].sort(key=lambda x:x['aug_logp'])\n",
    "#         done_beams[i] = done_beams[i][0:beam_ratio]\n",
    "    print(done_beams)\n",
    "    for beam in done_beams:\n",
    "        caption = []\n",
    "        for item in beam['seq']:\n",
    "            caption.append(rev_word_map[int(item)])\n",
    "        print(caption)\n",
    "    return done_beams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-mSMICQCrcI5"
   },
   "outputs": [],
   "source": [
    "def caption_image_beam_search(\n",
    "        encoder,\n",
    "        decoder,\n",
    "        image_path,\n",
    "        word_map,\n",
    "        beam_size,\n",
    "        num_groups,\n",
    "        penalty):\n",
    "    \"\"\"Reads an image and captions it with beam search.\n",
    "    param encoder: encoder model\n",
    "    param decoder: decoder model\n",
    "    param image_path: path to image\n",
    "    param word_map: word map\n",
    "    param beam_size: number of sequences to consider at each decode-step\n",
    "    return: caption, weights for visualization\n",
    "    \"\"\"\n",
    "\n",
    "    k = beam_size\n",
    "    beam_ratio = int(k/num_groups)\n",
    "    vocab_size = len(word_map)\n",
    "\n",
    "    # Read image and process\n",
    "    img = imread(image_path)\n",
    "\n",
    "    if len(img.shape) == 2:\n",
    "        img = img[:, :, np.newaxis]\n",
    "        img = np.concatenate([img, img, img], axis=2)\n",
    "    img = imresize(img, (256, 256))\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = img / 255.\n",
    "    img = torch.FloatTensor(img)\n",
    "    img = img.to(device)\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    transform = transforms.Compose([normalize])\n",
    "    image = transform(img)  # (3, 256, 256)\n",
    "\n",
    "    # Encode\n",
    "    image = img.unsqueeze(0)  # (1, 3, 256, 256)\n",
    "    encoder_out = encoder(image)\n",
    "                # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "    enc_image_size = encoder_out.size(1)\n",
    "    encoder_dim = encoder_out.size(3)\n",
    "\n",
    "    # Flatten encoding\n",
    "    encoder_out = encoder_out.view(1, -1, encoder_dim)\n",
    "    # (1, num_pixels, encoder_dim)\n",
    "\n",
    "    num_pixels = encoder_out.size(1)\n",
    "\n",
    "    # We'll treat the problem as having a batch size of k\n",
    "    encoder_out = encoder_out.expand(\n",
    "        beam_ratio,\n",
    "        num_pixels,\n",
    "        encoder_dim\n",
    "    )  # (k, num_pixels, encoder_dim)\n",
    "\n",
    "    # Tensor to store top k previous words at each step;\n",
    "    #now they're just <start>\n",
    "    # k_prev_words = torch.LongTensor(\n",
    "    #     [[word_map['<start>']]]\n",
    "    # ).to(device)  # (k, 1)\n",
    "\n",
    "    # init_logprobs = [([None] * beam_ratio)*num_groups]\n",
    "\n",
    "    h, c = decoder.init_hidden_state(encoder_out)\n",
    "#     print(\"h: \", h.shape)\n",
    "#     print(\"c: \", c.shape)\n",
    "\n",
    "    def generate_log_probabilities(inputs, states):\n",
    "        h_ = states[0]\n",
    "        c_ = states[1]\n",
    "        embeddings = decoder.embedding(inputs).squeeze(1)\n",
    "        awe, alpha = decoder.attention(encoder_out, h_)\n",
    "#         print(\"awe before gate*awe: \" + str(awe.shape))\n",
    "        gate = decoder.sigmoid(decoder.f_beta(h_))\n",
    "        awe = gate * awe\n",
    "#         print(\"awe after gate*awe: \" + str(awe.shape))\n",
    "        \n",
    "#         print(\"embeddings: \"+ str(embeddings.shape))\n",
    "#         print(\"concatenated: \",torch.cat([embeddings, awe], dim=1).size())\n",
    "        h_, c_ = decoder.decode_step(\n",
    "            torch.cat([embeddings, awe], dim=1),\n",
    "            (h_, c_)\n",
    "        )  #tried something\n",
    "        scores = F.log_softmax(decoder.fc(h_), dim=1)\n",
    "        return h_, c_, scores\n",
    "\n",
    "    init_seq = torch.LongTensor([[word_map['<start>']]]*beam_ratio).to(device)\n",
    "#     print(\"Sizeof Init_Seq: {}\".format(init_seq.size()))\n",
    "    # init_seq = torch.LongTensor([word_map['<start>']]*beam_ratio).to(device).unsqueeze(0)\n",
    "    h,c,init_logprobs = generate_log_probabilities(init_seq, [h,c])\n",
    "\n",
    "    done_beams = diverse_beam_search(\n",
    "        rnn_state=[h, c],\n",
    "        init_logprobs=init_logprobs,\n",
    "        num_beams=k,\n",
    "        num_groups=num_groups,\n",
    "        penalty_lambda=penalty,\n",
    "        seq_length=50,\n",
    "        end_token=word_map['<end>'],\n",
    "        gen_logprobs=generate_log_probabilities,\n",
    "        init_token=word_map['<start>']\n",
    "    )\n",
    "    # s is a number less than or equal to k,\n",
    "    #because sequences are removed from this process once they hit <end>\n",
    "#     for beam in done_beams:\n",
    "#         print(type(beam))\n",
    "#         print(beam)\n",
    "    seq = [beam['seq'] for beam in done_beams]\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hlNaW1G-tXDy"
   },
   "outputs": [],
   "source": [
    "model_path = \"./pretrained_model/BEST_checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar\"\n",
    "word_map_path = \"./pretrained_model/WORDMAP_coco_5_cap_per_img_5_min_word_freq.json\"\n",
    "image_path = \"./animal_khichdi.jpeg\"\n",
    "beam_size = 8 # keep num group == num beams, or prepare for trouble\n",
    "num_groups = 8 # and make it double\n",
    "_lambda = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZipMdnflwjWB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'seq': tensor([   1,   96,   17, 1377,   28,    1,   62,   17, 1052,   32,    1, 1473,\n",
      "         716, 9489,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0], device='cuda:0'), 'logps': tensor([-0.4918, -1.9705, -0.0045, -1.4801, -0.6780, -1.8856, -1.8331, -0.0140,\n",
      "        -1.6889, -1.1403, -0.5282, -1.4369, -0.8050, -0.0358,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000], device='cuda:0', grad_fn=<CloneBackward>), 'logp': tensor(-13.9927, device='cuda:0', grad_fn=<SumBackward0>), 'aug_logp': tensor(-1000053.8125, device='cuda:0', grad_fn=<AsStridedBackward>), 'candidate': {'column': tensor(9489, device='cuda:0'), 'row': 0, 'prob': tensor(-13.9927, device='cuda:0', grad_fn=<ThAddBackward>), 'local_logprob': tensor(-0.0358, device='cuda:0', grad_fn=<SelectBackward>), 'kept': True}}, {'seq': tensor([   1,   62,   17, 1301,   71,   61,   23,    1,   62,   17, 1052, 9489,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0], device='cuda:0'), 'logps': tensor([-0.4918, -2.0984, -0.0037, -1.9443, -1.6210, -1.1011, -0.0011, -0.4020,\n",
      "        -0.8185, -0.0015, -1.0845, -0.0578,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000], device='cuda:0', grad_fn=<CloneBackward>), 'logp': tensor(-9.6258, device='cuda:0', grad_fn=<SumBackward0>), 'aug_logp': tensor(-1000047.2500, device='cuda:0', grad_fn=<AsStridedBackward>), 'candidate': {'column': tensor(9489, device='cuda:0'), 'row': 0, 'prob': tensor(-10.6258, device='cuda:0', grad_fn=<ThAddBackward>), 'local_logprob': tensor(-0.0578, device='cuda:0', grad_fn=<SelectBackward>), 'kept': True}}, {'seq': tensor([   1,   96,   17, 1377,   28, 1049,   32,    1,  716, 9489,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0], device='cuda:0'), 'logps': tensor([-0.4918, -1.9705, -0.0045, -1.4801, -0.6780, -1.9303, -0.8922, -0.3848,\n",
      "        -1.3894, -0.2450,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000], device='cuda:0', grad_fn=<CloneBackward>), 'logp': tensor(-9.4666, device='cuda:0', grad_fn=<SumBackward0>), 'aug_logp': tensor(-1000062.3750, device='cuda:0', grad_fn=<AsStridedBackward>), 'candidate': {'column': tensor(9489, device='cuda:0'), 'row': 0, 'prob': tensor(-13.4666, device='cuda:0', grad_fn=<ThAddBackward>), 'local_logprob': tensor(-0.2450, device='cuda:0', grad_fn=<SelectBackward>), 'kept': True}}, {'seq': tensor([   1,   62,   17, 1553, 1049,  391,  335,    1,  810,  547,  587, 9489,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0], device='cuda:0'), 'logps': tensor([-0.4918, -2.0984, -0.0037, -2.2868, -0.2860, -1.3361, -1.4560, -0.1446,\n",
      "        -0.1348, -0.2267, -0.5536, -0.0215,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000], device='cuda:0', grad_fn=<CloneBackward>), 'logp': tensor(-9.0401, device='cuda:0', grad_fn=<SumBackward0>), 'aug_logp': tensor(-1000055.1875, device='cuda:0', grad_fn=<AsStridedBackward>), 'candidate': {'column': tensor(9489, device='cuda:0'), 'row': 0, 'prob': tensor(-14.0401, device='cuda:0', grad_fn=<ThAddBackward>), 'local_logprob': tensor(-0.0215, device='cuda:0', grad_fn=<SelectBackward>), 'kept': True}}, {'seq': tensor([ 164,  216, 1049,  179,  391,   32,   14,  810, 9489,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0], device='cuda:0'), 'logps': tensor([-2.3654, -1.7406, -0.9607, -1.3736, -2.1637, -1.2939, -0.4343, -0.1858,\n",
      "        -0.0169,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000], device='cuda:0', grad_fn=<CloneBackward>), 'logp': tensor(-10.5349, device='cuda:0', grad_fn=<SumBackward0>), 'aug_logp': tensor(-1000080.1250, device='cuda:0', grad_fn=<AsStridedBackward>), 'candidate': {'column': tensor(9489, device='cuda:0'), 'row': 0, 'prob': tensor(-10.5349, device='cuda:0', grad_fn=<ThAddBackward>), 'local_logprob': tensor(-0.0169, device='cuda:0', grad_fn=<SelectBackward>), 'kept': True}}, {'seq': tensor([   1,  339,   17, 1049,   50,  179,   71,   32,   14,  810, 9489,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0], device='cuda:0'), 'logps': tensor([-0.4918, -2.6603, -0.0028, -1.3981, -0.8900, -0.0261, -1.4914, -0.4342,\n",
      "        -0.1567, -0.0226, -0.0061,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000], device='cuda:0', grad_fn=<CloneBackward>), 'logp': tensor(-7.5802, device='cuda:0', grad_fn=<SumBackward0>), 'aug_logp': tensor(-1000061.1250, device='cuda:0', grad_fn=<AsStridedBackward>), 'candidate': {'column': tensor(9489, device='cuda:0'), 'row': 0, 'prob': tensor(-12.0802, device='cuda:0', grad_fn=<ThAddBackward>), 'local_logprob': tensor(-0.0061, device='cuda:0', grad_fn=<SelectBackward>), 'kept': True}}, {'seq': tensor([ 175,  216, 1049,   32,    1,  716,    3,  164, 1112, 9489,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0], device='cuda:0'), 'logps': tensor([-2.7487, -2.2444, -0.8604, -1.9472, -0.6314, -2.6105, -1.0556, -2.2113,\n",
      "        -2.3608, -0.4861,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000], device='cuda:0', grad_fn=<CloneBackward>), 'logp': tensor(-17.1564, device='cuda:0', grad_fn=<SumBackward0>), 'aug_logp': tensor(-1000060.1250, device='cuda:0', grad_fn=<AsStridedBackward>), 'candidate': {'column': tensor(9489, device='cuda:0'), 'row': 0, 'prob': tensor(-19.1564, device='cuda:0', grad_fn=<ThAddBackward>), 'local_logprob': tensor(-0.4861, device='cuda:0', grad_fn=<SelectBackward>), 'kept': True}}, {'seq': tensor([ 164,  216, 1049,  179,   32,   14,  810, 9489,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0], device='cuda:0'), 'logps': tensor([-2.3654, -1.7406, -0.9607, -1.3736, -2.2958, -0.6374, -0.9453, -1.8007,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000], device='cuda:0', grad_fn=<CloneBackward>), 'logp': tensor(-12.1195, device='cuda:0', grad_fn=<SumBackward0>), 'aug_logp': tensor(-1000088.2500, device='cuda:0', grad_fn=<AsStridedBackward>), 'candidate': {'column': tensor(9489, device='cuda:0'), 'row': 0, 'prob': tensor(-15.1195, device='cuda:0', grad_fn=<ThAddBackward>), 'local_logprob': tensor(-1.8007, device='cuda:0', grad_fn=<SelectBackward>), 'kept': True}}]\n",
      "['a', 'group', 'of', 'zebras', 'and', 'a', 'herd', 'of', 'deer', 'in', 'a', 'snowy', 'field', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['a', 'herd', 'of', 'zebra', 'standing', 'next', 'to', 'a', 'herd', 'of', 'deer', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['a', 'group', 'of', 'zebras', 'and', 'animals', 'in', 'a', 'field', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['a', 'herd', 'of', 'wild', 'animals', 'walking', 'across', 'a', 'snow', 'covered', 'ground', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['many', 'different', 'animals', 'are', 'walking', 'in', 'the', 'snow', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['a', 'bunch', 'of', 'animals', 'that', 'are', 'standing', 'in', 'the', 'snow', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['several', 'different', 'animals', 'in', 'a', 'field', 'with', 'many', 'sheep', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['many', 'different', 'animals', 'are', 'in', 'the', 'snow', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "a group of zebras and a herd of deer in a snowy field \n",
      "a herd of zebra standing next to a herd of deer \n",
      "a group of zebras and animals in a field \n",
      "a herd of wild animals walking across a snow covered ground \n",
      "many different animals are walking in the snow \n",
      "a bunch of animals that are standing in the snow \n",
      "several different animals in a field with many sheep \n",
      "many different animals are in the snow \n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(model_path)\n",
    "decoder = checkpoint['decoder']\n",
    "decoder = decoder.to(device)\n",
    "decoder.eval()\n",
    "encoder = checkpoint['encoder']\n",
    "encoder = encoder.to(device)\n",
    "encoder.eval()\n",
    "\n",
    "with open(word_map_path, 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "rev_word_map = {v: k for k, v in word_map.items()}\n",
    "seqs = caption_image_beam_search(\n",
    "        encoder,\n",
    "        decoder,\n",
    "        image_path,\n",
    "        word_map,\n",
    "        beam_size=beam_size,\n",
    "        num_groups=num_groups,\n",
    "        penalty=_lambda\n",
    "    )\n",
    "\n",
    "for seq in seqs:\n",
    "    word = ''\n",
    "    words = \"\"\n",
    "    for ind in seq:\n",
    "        word = rev_word_map[int(ind)]\n",
    "        if word == '<end>':\n",
    "            break        \n",
    "        words+= (word + \" \")\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1WKegyYdFPOc"
   },
   "outputs": [],
   "source": [
    "a = torch.LongTensor([[1,2],[3,4],[5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOC8pfyrLLf0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(a[2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3,4,5,6],[1,2,3,4,5,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(a[:,5],6).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2],\n",
    "[3,4]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "DiverseBeamSearchCaption.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
